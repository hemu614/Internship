# -*- coding: utf-8 -*-
"""Project discussion 1_classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wa5u3syUnieLugPI66uTHNMypvRyBNr0
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('https://raw.githubusercontent.com/dsrscientist/DSData/master/Telecom_customer_churn.csv')
df

"""This DataSet the details of coustomer in which both numerical and coegorial data are present. Here 'churn' is target variable  which contains 2 catogeries so it will be "Classsification Problem" where we need to predic several churn using classification model.

# Exploratory Data Analysis
"""

df.shape

"""This data set contains 7043 raw and 21 columns in which 1 is target variable and other is independent variable"""

df.info()

df.loc[df['TotalCharges']== " "]

df['TotalCharges']= df['TotalCharges'].replace(' ',np.nan)

df.isnull().sum()

df["TotalCharges"] = df["TotalCharges"].astype(float)

np.mean(df["TotalCharges"])

df["TotalCharges"].fillna(np.mean(df["TotalCharges"]),inplace=True)

df.iloc[488,:]

cat = []
for i in df.dtypes.index:
  if df.dtypes[i]== 'object':
    cat.append(i)

print(cat)

num = []
for i in df.dtypes.index:
  if df.dtypes[i] != 'object':
    num.append(i)

print(num)

df['Churn'].value_counts()

df.drop('customerID',axis = 1, inplace = True)

df.nunique().to_frame("Unique")

"""# Description Of Dataset"""

#statestical summary of numerical columns
df.describe()

"""75% and max have huge diffrence meanse outlier is present

mean < meadian(50%) =  left skwed ::  monthlycharges

mean > meadin (50%) = right skewed :: tenure, seniorcitizen , Totalcharges

# **Data Visulisation **

Univariate analysis
"""

ax = sns.countplot(x = 'Churn', data = df)
print(df['Churn'].value_counts())

plt.figure(figsize=(10,5), facecolor='white')
plotnumber = 1
for col in num:
  if plotnumber <=8:
    plt.subplot(3,3,plotnumber)
    sns.distplot(df[col])
    plt.xlabel(col)
    plt.yticks(rotation=0)

  plotnumber +=1

plt.tight_layout()

"""from above dist plot we can conclude tha totalcharges and senior citizen are skew toward left"""

sns.stripplot(x='SeniorCitizen', y='tenure',data=df)
plt.show

sns.scatterplot(x='tenure',y = 'TotalCharges',data=df, hue='Churn')
plt.show()

"""we can see strong linear relation between features
as tenure increases total charges also increases
if coustomer have low churn there is high chance of Churn(target variable)
"""

sns.barplot(x='gender',y='SeniorCitizen',data=df, hue='Churn', palette = 'winter_r')
plt.show()

sns.scatterplot(x='TotalCharges',y='MonthlyCharges',data=df, hue ='Churn')
plt.show()

sns.catplot(x='Churn',col= 'gender',kind='count',data=df)

sns.pairplot(df,hue='Churn')
plt.show()

"""1. Pairplot gives pairwise relationship between feature and target'Churn'., digonal plot shows distribusion plot

2. we can also check linear relation with each other

"""

plt.figure(figsize=(10,10), facecolor = 'white')
plotnumber=1
for col in df:
  if plotnumber <=8:
    plt.subplot(3,3,plotnumber)
    sns.boxplot(df[col])
    plt.xlabel(col)
    plt.yticks(rotation=0)
  plotnumber +=1

plt.tight_layout()

"""Senior citizen have oulier but its catogrical column so need to remove outlier

# Encoding Categorical columns
"""

from sklearn.preprocessing import OrdinalEncoder
OE = OrdinalEncoder()
for i in df.columns:
  if df[i].dtypes == 'object':
    df[i] = OE.fit_transform(df[i].values.reshape(-1,1))

df.skew()

df.corr()

plt.figure(figsize=(20,12))
sns.heatmap(df.corr(),annot = True, linewidth=0.1, linecolor= "Black")
plt.show()

"""0 - no corelaation

-0.3 to +0.3 = less corelation

-0.5 to 0.5 = modrate corealtion

-0.7 to +0.7 = high corelation
"""

#seaprating features and lable
x = df.drop('Churn', axis=1)
y = df['Churn']

"""# Seaprating Features and Lable"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x= pd.DataFrame(sc.fit_transform(x),columns=x.columns)
x

"""# Feature scaling and standard scalisation"""

#seaprating features and lable
x = df.drop('Churn', axis=1)
y = df['Churn']

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x= pd.DataFrame(sc.fit_transform(x),columns=x.columns)
x

"""### Cheacking Variance Inflation Factor(VIF)
It gives multi colinearity
"""

from statsmodels.stats.outliers_influence import variance_inflation_factor
vif = pd.DataFrame()
vif['VIF value'] = [variance_inflation_factor(x.values,i) for i in range(len(x.columns))]
vif['Features'] = x.columns
vif

"""High VIF means high colinearity with other featires so we will drop the column by checking high VIF and less correlationg with lable('Churn').

we are droping total charges column here
"""

df.drop('TotalCharges',axis=1,inplace = True )

vif = pd.DataFrame()
vif['VIF value'] = [variance_inflation_factor(x.values,i) for i in range(len(x.columns))]
vif['Features'] = x.columns
vif

"""# Oversampling
its uses for class imbalnce issue
"""

from imblearn.over_sampling import SMOTE
sm = SMOTE()
x1,y1 = sm.fit_resample(x,y)

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score


from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, accuracy_score
from sklearn.model_selection import cross_val_score

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=.3,random_state=42)

lg= LogisticRegression()
lg.fit(x_train,y_train)
lg.score(x_train,y_train)
predm=lg.predict(x_test)
print("accuracy score",lg,"is")
print(accuracy_score(y_test,predm)*100)
print(confusion_matrix(y_test,predm))
print(classification_report(y_test,predm))
print('\n')

lg=LogisticRegression()
RFC=RandomForestClassifier()
ETC=ExtraTreesClassifier()
GBC=GradientBoostingClassifier()
ABC =AdaBoostClassifier()
BC=BaggingClassifier()

model=[lg,RFC,ETC,GBC,ABC,BC]

for m in model:

  m.fit(x_train,y_train)
  m.score(x_train,y_train)
  predm=m.predict(x_test)
  print("accuracy score",m,"is")
  print(accuracy_score(y_test,predm)*100)
  print(confusion_matrix(y_test,predm))
  print(classification_report(y_test,predm))
  print('\n')

from sklearn.model_selection import cross_val_score
from sklearn.metrics import accuracy_score

lg = LogisticRegression()
RFC = RandomForestClassifier()
ETC = ExtraTreesClassifier()
GBC = GradientBoostingClassifier()
ABC = AdaBoostClassifier()
BC = BaggingClassifier()

model = [lg, RFC, ETC,ABC, BC]

for i in model:
  i.fit(x_train,y_train)
  score = cross_val_score(i, x, y)
  print(score)
  print(score.mean())

  # Make predictions on the test set
  pred = i.predict(x_test)

  # Calculate and print the difference between accuracy on test set and cross-validation mean
  print("Difference for", i, "model:", accuracy_score(y_test, pred) - score.mean())
  print('\n')

"""We will select model whos diffrence between cross val score and accuracy is least"""

from sklearn.model_selection import GridSearchCV

parameter={

    'penalty':['l1', 'l2', 'elasticnet'],
    'C':[10,50,100],
    'intercept_scaling':[0,10,20],
    'n_jobs':[-2,-1,1],
    'solver':['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']

}

GCV=GridSearchCV(LogisticRegression(),parameter,cv=5)

GCV.fit(x_train, y_train)

GCV.best_params_

Final_model = LogisticRegression(solver= 'liblinear',
 intercept_scaling = 10,
 n_jobs= -2,
 C = 10)

import joblib
joblib.dump(Final_model,'Telecom_churn.pkl')

model=joblib.load('Telecom_churn.pkl')

model.fit(x_train, y_train)
predictions = model.predict(x_test)

a=np.array(y_test)
df_pred=pd.DataFrame()
df_pred['predicted']= predictions
df_pred['original']=a
df_pred

sns.scatterplot(x='predicted',y = 'original',data=df_pred)
plt.show()

