# -*- coding: utf-8 -*-
"""Project 2_regression_Class.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XCwz4wOF-HpeImLTiIJx4oqT7faxXfJF
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

df = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/tesla-stock-price.csv')
df

df.head()

df.tail()

print(f"the Dimention of dataset",df.shape)
print(f"\n the volume header in dataset,{df.columns}")
print("min date: ", df['date'].min())
print("max date: ", df['date'].max())

df.dtypes

df.drop(index=df.index[0], axis=0, inplace=True)

df

df.isnull().sum()

df.set_index('date',inplace=True)

df

df.describe()

sns.pairplot(data = df, palette='colorblid')

sns.lmplot(x='open',y='close',data =df, palette='colorblind')

sns.boxplot(x='open',y='close',data =df)

from sklearn.model_selection import train_test_split
from sklearn.metrics  import mean_absolute_error
from sklearn.metrics import mean_squared_error, r2_score


from sklearn.linear_model import LinearRegression, Lasso,Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsRegressor as KNN

x = df.drop('open',axis=1)
y = df['open']

x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=.3,random_state=42)

from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()
x=pd.DataFrame(scaler.fit_transform(x), columns= x.columns)

from statsmodels.stats.outliers_influence import variance_inflation_factor

vif_Tmin =pd.DataFrame()
vif_Tmin["VIF Values"]= [variance_inflation_factor(x.values,i)for i in range (len(x.columns))]
vif_Tmin

lr=LinearRegression()
RFR=RandomForestRegressor()
DTC=DecisionTreeClassifier()
knn=KNN()
las =Lasso()
rid=Ridge()

model=[lr,RFR,knn,las,rid]

for m in model:

  m.fit(x_train,y_train)
  m.score(x_train,y_train)
  predm=m.predict(x_test)
  print("r2_score",m,"is")
  print(r2_score(y_test,predm)*100)
  print(mean_absolute_error(y_test,predm))
  print(mean_squared_error(y_test,predm))
  print('\n')

from sklearn.model_selection import cross_val_score

lr=LinearRegression()
RFR=RandomForestRegressor()
DTC=DecisionTreeClassifier()
knn=KNN()
las =Lasso()
rid=Ridge()

model = [lr,RFR,knn,las,rid]

for i in model:
  i.fit(x_train,y_train)
  score = cross_val_score(i, x, y)
  print(score)
  print(score.mean())

  # Make predictions on the test set
  pred = i.predict(x_test)

  # Calculate and print the difference between accuracy on test set and cross-validation mean
  print("Difference for", i, "model:", r2_score(y_test, pred) - score.mean())
  print('\n')

param_grid = {

              'criterion' : ['squared_error', 'absolute_error', 'friedman_mse', 'poisson'],
              'max_depth':[0,10,20],
              'random_state':[10,50,100],
              'n_jobs':[-2,-1,1],
              'n_estimators':[50,100,200,300]

}

from sklearn.model_selection import GridSearchCV

GCV=GridSearchCV(RandomForestRegressor(),param_grid,cv=5)

GCV.fit(x_train,y_train)

